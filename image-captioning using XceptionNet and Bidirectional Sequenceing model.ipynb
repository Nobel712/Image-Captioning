{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":1949071,"sourceType":"datasetVersion","datasetId":1162937},{"sourceId":3163379,"sourceType":"datasetVersion","datasetId":1923811}],"dockerImageVersionId":30157,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Firstly, we have to import the libraries required as follows:","metadata":{}},{"cell_type":"code","source":"from os import listdir\nimport tensorflow as tf\nfrom pickle import dump\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import Xception,MobileNetV3Large,VGG16\nfrom tensorflow.keras.applications.xception import preprocess_input\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input, Embedding\nfrom tensorflow.keras.layers import LSTM, Dropout\nfrom keras.layers.merge import add\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import plot_model\nimport string\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-12-10T23:55:57.729205Z","iopub.execute_input":"2023-12-10T23:55:57.729433Z","iopub.status.idle":"2023-12-10T23:55:58.019265Z","shell.execute_reply.started":"2023-12-10T23:55:57.729404Z","shell.execute_reply":"2023-12-10T23:55:58.018653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_Xception = Sequential()\nmodel_Xception.add(VGG16(include_top=False, weights='imagenet', pooling='max'))\nmodel_Xception.add(Dropout(0.2))\nmodel_Xception.add(Dense(2048, activation='relu'))\nmodel_Xception.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T23:55:58.020350Z","iopub.execute_input":"2023-12-10T23:55:58.020624Z","iopub.status.idle":"2023-12-10T23:56:01.551416Z","shell.execute_reply.started":"2023-12-10T23:55:58.020588Z","shell.execute_reply":"2023-12-10T23:56:01.550599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfil = '../input/d/sayanf/flickr8k/Flickr8k_Dataset' + '/' + '1000268201_693b08cb0e.jpg'\nima = load_img(fil, target_size=(299, 299))\nima = img_to_array(ima)\nima = ima.reshape((ima.shape[0], ima.shape[1], ima.shape[2]))\nima = preprocess_input(ima)\nplt.imshow(ima)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T23:56:01.552475Z","iopub.execute_input":"2023-12-10T23:56:01.552670Z","iopub.status.idle":"2023-12-10T23:56:01.860306Z","shell.execute_reply.started":"2023-12-10T23:56:01.552646Z","shell.execute_reply":"2023-12-10T23:56:01.859556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract features from each photo in the directory\ndef extract_features(directory):\n    # extract features from each photo\n    features = dict()\n    for name in listdir(directory):\n        # load an image from file\n        filename = directory + '/' + name\n        image = load_img(filename, target_size=(299, 299))\n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        # prepare the image for the Xception model\n        image = preprocess_input(image)\n        # get features\n        feature =model_Xception.predict(image, verbose=0)\n        # get image id\n        image_id = name.split('.')[0]\n        # store feature\n        features[image_id] = feature\n    return features","metadata":{"id":"dHIQa5NB1IvD","execution":{"iopub.status.busy":"2023-12-10T23:56:01.862135Z","iopub.execute_input":"2023-12-10T23:56:01.862388Z","iopub.status.idle":"2023-12-10T23:56:01.869383Z","shell.execute_reply.started":"2023-12-10T23:56:01.862358Z","shell.execute_reply":"2023-12-10T23:56:01.868685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract features from all images\ndirectory = '../input/d/sayanf/flickr8k/Flickr8k_Dataset'\nfeatures = extract_features(directory)\nprint('Extracted Features: %d' % len(features))\n# save to file\ndump(features, open('features.pkl', 'wb'))","metadata":{"id":"FoUQWxlU1Ip9","execution":{"iopub.status.busy":"2023-12-10T23:56:01.870193Z","iopub.execute_input":"2023-12-10T23:56:01.870381Z","iopub.status.idle":"2023-12-11T00:03:38.127030Z","shell.execute_reply.started":"2023-12-10T23:56:01.870358Z","shell.execute_reply":"2023-12-11T00:03:38.126263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n \nfilename = '../input/d/sayanf/flickr8k/Flickr8k_text/Flickr8k.token.txt'\n# load descriptions\ndoc = load_doc(filename)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:03:38.128346Z","iopub.execute_input":"2023-12-11T00:03:38.128628Z","iopub.status.idle":"2023-12-11T00:03:38.190468Z","shell.execute_reply.started":"2023-12-11T00:03:38.128589Z","shell.execute_reply":"2023-12-11T00:03:38.189713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract descriptions for images\ndef load_descriptions(doc):\n    mapping = dict()\n    # process lines\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        if len(line) < 2:\n            continue\n        # take the first token as the image id, the rest as the description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # remove filename from image id\n        image_id = image_id.split('.')[0]\n        # convert description tokens back to string\n        image_desc = ' '.join(image_desc)\n        # create the list if needed\n        if image_id not in mapping:\n            mapping[image_id] = list()\n        # store description\n        mapping[image_id].append(image_desc)\n    return mapping\n \n# parse descriptions\ndescriptions = load_descriptions(doc)\nprint('Number of images with their 5 descriptions loaded: %d ' % len(descriptions))","metadata":{"id":"GsGJAFcB1Tj-","execution":{"iopub.status.busy":"2023-12-11T00:03:38.191463Z","iopub.execute_input":"2023-12-11T00:03:38.191666Z","iopub.status.idle":"2023-12-11T00:03:38.314291Z","shell.execute_reply.started":"2023-12-11T00:03:38.191641Z","shell.execute_reply":"2023-12-11T00:03:38.313556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import load_img\n\ndir_descriptions = \"../input/d/sayanf/flickr8k/Flickr8k_text/Flickr8k.token.txt\"\ndir_imgs = \"../input/d/sayanf/flickr8k/Flickr8k_Dataset\"\nnpix = 299\ntarget_size = (npix,npix,3)\n\ncount = 1\nfig = plt.figure(figsize=(10,20))\nfor img in listdir(dir_imgs)[:5]:\n    filename = dir_imgs + '/' + img\n    captions = descriptions[img.split('.')[0]]\n    image_load = load_img(filename, target_size=target_size)\n    \n    ax = fig.add_subplot(5, 2, count, xticks=[], yticks=[])\n    ax.imshow(image_load)\n    count += 1\n    \n    ax = fig.add_subplot(5, 2, count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,len(captions))\n    for i, caption in enumerate(captions):\n        ax.text(0,i,caption,fontsize=20)\n    count += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:03:38.315411Z","iopub.execute_input":"2023-12-11T00:03:38.315682Z","iopub.status.idle":"2023-12-11T00:03:39.256215Z","shell.execute_reply.started":"2023-12-11T00:03:38.315642Z","shell.execute_reply":"2023-12-11T00:03:39.255454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter \nimport pandas as pd\n\ndef df_word(desc_dict):\n    vocabulary = []\n    for i in listdir(dir_imgs):\n        for j in range(5):\n            temp=descriptions[i.split('.')[0]][j]\n            vocabulary.extend(temp.split())\n    print('Vocabulary Size: %d' % len(set(vocabulary)))\n    ct = Counter(vocabulary)\n    dfword = pd.DataFrame({\"word\":list(ct.keys()),\"count\":list(ct.values())})\n    dfword = dfword.sort_values(\"count\",ascending=False)\n    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n    return(dfword)\n\ndfword = df_word(descriptions)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:03:39.257436Z","iopub.execute_input":"2023-12-11T00:03:39.257692Z","iopub.status.idle":"2023-12-11T00:03:39.441480Z","shell.execute_reply.started":"2023-12-11T00:03:39.257660Z","shell.execute_reply":"2023-12-11T00:03:39.440698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfword","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:03:39.442581Z","iopub.execute_input":"2023-12-11T00:03:39.442823Z","iopub.status.idle":"2023-12-11T00:03:39.457205Z","shell.execute_reply.started":"2023-12-11T00:03:39.442785Z","shell.execute_reply":"2023-12-11T00:03:39.456389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topn = 50\ndef plthist(dfsub, title=''):\n    plt.figure(figsize=(20,3))\n    plt.bar(dfsub.index,dfsub[\"count\"])\n    plt.yticks(fontsize=20)\n    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=80,fontsize=20)\n    plt.title(title,fontsize=20)\n    plt.show()\n\nplthist(dfword.iloc[:topn,:], title=\"Top 50 most frequent words\")\nplthist(dfword.iloc[-topn:,:], title=\"Top 50 less frequent words\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:03:39.458231Z","iopub.execute_input":"2023-12-11T00:03:39.458468Z","iopub.status.idle":"2023-12-11T00:03:40.719505Z","shell.execute_reply.started":"2023-12-11T00:03:39.458442Z","shell.execute_reply":"2023-12-11T00:03:40.718689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n \ndef clean_descriptions(descriptions):\n    # prepare translation table for removing punctuation\n    table = str.maketrans('', '', string.punctuation)\n    for key, desc_list in descriptions.items():\n        #for i in range(len(desc_list)):\n        for i in range(len(desc_list)):\n            desc = desc_list[i]\n            # tokenize\n            desc = desc.split()\n            # convert to lower case\n            desc = [word.lower() for word in desc]\n            # remove punctuation from each token\n            desc = [w.translate(table) for w in desc]\n            # remove hanging 's' and 'a'\n            desc = [word for word in desc if len(word)>1]\n            # remove tokens with numbers in them\n            desc = [word for word in desc if word.isalpha()]\n            # store as string\n            desc_list[i] =  ' '.join(desc)\n\n\n# Making use of the function just created\nclean_descriptions(descriptions)","metadata":{"id":"Be-KKpav1TcJ","execution":{"iopub.status.busy":"2023-12-11T00:03:40.720747Z","iopub.execute_input":"2023-12-11T00:03:40.721300Z","iopub.status.idle":"2023-12-11T00:03:41.242898Z","shell.execute_reply.started":"2023-12-11T00:03:40.721259Z","shell.execute_reply":"2023-12-11T00:03:41.242116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the loaded descriptions into a vocabulary of words\ndef to_vocabulary(descriptions):\n    # build a list of all description strings\n    all_desc = set()\n    for key in descriptions.keys():\n        [all_desc.update(d.split()) for d in descriptions[key]]\n    return all_desc\n \n# summarize vocabulary\nvocabulary = to_vocabulary(descriptions)\nprint('Vocabulary Size: %d' % len(vocabulary))","metadata":{"id":"_a1mFQrc8C7F","execution":{"iopub.status.busy":"2023-12-11T00:03:41.245675Z","iopub.execute_input":"2023-12-11T00:03:41.245915Z","iopub.status.idle":"2023-12-11T00:03:41.315212Z","shell.execute_reply.started":"2023-12-11T00:03:41.245888Z","shell.execute_reply":"2023-12-11T00:03:41.314530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n    lines = list()\n    for key, desc_list in descriptions.items():\n        #for desc in desc_list:   # If you have RAM higher than 16GB uncomment this line\n        for desc in desc_list[:2]:  # This will make our program to save only 2 captions per image\n            lines.append(key + ' ' + desc)\n    data = '\\n'.join(lines)\n    file = open(filename, 'w')\n    file.write(data)\n    file.close()\n\n# save descriptions\nsave_descriptions(descriptions, 'descriptions.txt')","metadata":{"id":"90Ar5Elb8Cxe","execution":{"iopub.status.busy":"2023-12-11T00:03:41.316134Z","iopub.execute_input":"2023-12-11T00:03:41.316354Z","iopub.status.idle":"2023-12-11T00:03:41.331925Z","shell.execute_reply.started":"2023-12-11T00:03:41.316327Z","shell.execute_reply":"2023-12-11T00:03:41.331120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load a pre-defined list of photo identifiers\ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    # process line by line\n    for line in doc.split('\\n'):\n        # skip empty lines\n        if len(line) < 1:\n            continue\n        # get the image identifier\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:03:41.333035Z","iopub.execute_input":"2023-12-11T00:03:41.333240Z","iopub.status.idle":"2023-12-11T00:03:41.341176Z","shell.execute_reply.started":"2023-12-11T00:03:41.333214Z","shell.execute_reply":"2023-12-11T00:03:41.340451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n    # load document\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        # split id from description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # skip images not in the set\n        if image_id in dataset:\n            # create list\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            # wrap description in tokens\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            # store\n            descriptions[image_id].append(desc)\n    return descriptions","metadata":{"id":"JAb-V5nL8sSe","execution":{"iopub.status.busy":"2023-12-11T00:03:41.342140Z","iopub.execute_input":"2023-12-11T00:03:41.342397Z","iopub.status.idle":"2023-12-11T00:03:41.351891Z","shell.execute_reply.started":"2023-12-11T00:03:41.342358Z","shell.execute_reply":"2023-12-11T00:03:41.351198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load photo features\ndef load_photo_features(filename, dataset):\n    # load all features\n    all_features = load(open(filename, 'rb'))\n    # filter features\n    features = {k: all_features[k] for k in dataset}\n    return features","metadata":{"id":"8VGROgI88sMG","execution":{"iopub.status.busy":"2023-12-11T00:03:41.352910Z","iopub.execute_input":"2023-12-11T00:03:41.353173Z","iopub.status.idle":"2023-12-11T00:03:41.365973Z","shell.execute_reply.started":"2023-12-11T00:03:41.353138Z","shell.execute_reply":"2023-12-11T00:03:41.365306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n# convert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n \n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","metadata":{"id":"qE9_OV0x8sDL","execution":{"iopub.status.busy":"2023-12-11T00:03:41.366911Z","iopub.execute_input":"2023-12-11T00:03:41.367154Z","iopub.status.idle":"2023-12-11T00:03:41.377194Z","shell.execute_reply.started":"2023-12-11T00:03:41.367121Z","shell.execute_reply":"2023-12-11T00:03:41.376435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the length of the description with the most words\ndef max_length(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)","metadata":{"id":"dGNM6BFM_Bw7","execution":{"iopub.status.busy":"2023-12-11T00:03:41.378046Z","iopub.execute_input":"2023-12-11T00:03:41.378243Z","iopub.status.idle":"2023-12-11T00:03:41.387511Z","shell.execute_reply.started":"2023-12-11T00:03:41.378216Z","shell.execute_reply":"2023-12-11T00:03:41.386847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\n# create sequences of images, input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n    X1, X2, y = list(), list(), list()\n    # walk through each image identifier\n    for key, desc_list in descriptions.items():\n        # walk through each description for the image\n        for desc in desc_list:\n            # encode the sequence\n            seq = tokenizer.texts_to_sequences([desc])[0]\n            # split one sequence into multiple X,y pairs\n            for i in range(1, len(seq)):\n                # split into input and output pair\n                in_seq, out_seq = seq[:i], seq[i]\n                # pad input sequence\n                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                # encode output sequence\n                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                # store\n                X1.append(photos[key][0])\n                X2.append(in_seq)\n                y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)","metadata":{"id":"Vnjc66i-8r2s","execution":{"iopub.status.busy":"2023-12-11T00:03:41.388401Z","iopub.execute_input":"2023-12-11T00:03:41.388601Z","iopub.status.idle":"2023-12-11T00:03:41.399958Z","shell.execute_reply.started":"2023-12-11T00:03:41.388577Z","shell.execute_reply":"2023-12-11T00:03:41.399276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import RepeatVector, Concatenate, Activation, Dot, Permute, Bidirectional, LSTM\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, Reshape\nimport keras.backend as K\n\n# Define the captioning model with attention and Bi-LSTM\ndef define_model_with_attention_bi_lstm(vocab_size, max_length):\n    # Feature extractor model\n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(128, activation='relu')(fe1)\n\n    # Sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 128, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = Bidirectional(LSTM(128, return_sequences=True))(se2)\n\n    # Attention mechanism\n    fe2_repeated = RepeatVector(max_length)(fe2)\n    fe2_repeated = Reshape((-1, 128))(fe2_repeated)\n\n    attention_concat = Concatenate(axis=-1)([se3, fe2_repeated])\n    attention_dense = Dense(1, activation='tanh')(attention_concat)\n    attention_vector = Activation('softmax')(attention_dense)\n    attention_vector = Permute((2, 1))(attention_vector)\n\n    context = Dot(axes=[2, 1])([attention_vector, se3])\n    context = Bidirectional(LSTM(128))(context)\n\n    # Decoder model\n    decoder_input = Concatenate(axis=-1)([context, fe2])\n    decoder1 = Dense(256, activation='relu')(decoder_input)\n    outputs = Dense(vocab_size, activation='softmax')(decoder1)\n\n    # Tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Summarize model\n \n\n    return model\n\n# Create the model with attention and Bi-LSTM\nmodel = define_model_with_attention_bi_lstm(5200, 17)\nmodel.summary()\n","metadata":{"id":"rvbrMAF69OZI","execution":{"iopub.status.busy":"2023-12-11T00:08:32.446307Z","iopub.execute_input":"2023-12-11T00:08:32.447101Z","iopub.status.idle":"2023-12-11T00:08:34.684994Z","shell.execute_reply.started":"2023-12-11T00:08:32.447061Z","shell.execute_reply":"2023-12-11T00:08:34.683862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train dataset","metadata":{}},{"cell_type":"code","source":"from pickle import load\n\n# load training dataset (6K)\nfilename = '../input/d/sayanf/flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n\n# descriptions\ntrain_descriptions = load_clean_descriptions('./descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n\n# photo features\ntrain_features = load_photo_features('./features.pkl', train)\n#train_features = load(open('./features.pkl', 'rb'))\nprint('Photos: train=%d' % len(train_features))\n\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n\n# determine the maximum sequence length\n#max_length = max_length(train_descriptions)\nmax_length=17\nprint('Description Length: %d' % max_length)\n\nlengths=[]\nlines = to_lines(train_descriptions)\nfor d in lines:\n    lengths.append(len(d.split())) \n    \nplt.hist(lengths, bins=28, alpha=0.5)\nplt.title('Distribution of captions length')\nplt.show()\n\n# prepare sequences\nX1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)","metadata":{"id":"yHxcdjSu_RzS","execution":{"iopub.status.busy":"2023-12-11T00:08:36.586198Z","iopub.execute_input":"2023-12-11T00:08:36.586742Z","iopub.status.idle":"2023-12-11T00:08:44.729410Z","shell.execute_reply.started":"2023-12-11T00:08:36.586706Z","shell.execute_reply":"2023-12-11T00:08:44.728711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load test set\nfilename = '../input/d/sayanf/flickr8k/Flickr8k_text/Flickr_8k.devImages.txt'\ndev = load_set(filename)\nprint('Dataset: %d' % len(dev))\n# descriptions\ndev_descriptions = load_clean_descriptions('./descriptions.txt', dev)\nprint('Descriptions: test=%d' % len(dev_descriptions))\n# photo features\ndev_features = load_photo_features('./features.pkl', dev)\nprint('Photos: test=%d' % len(dev_features))\n# prepare sequences\nX1dev, X2dev, ydev = create_sequences(tokenizer, max_length, dev_descriptions, dev_features, vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:08:44.731424Z","iopub.execute_input":"2023-12-11T00:08:44.731777Z","iopub.status.idle":"2023-12-11T00:08:45.855929Z","shell.execute_reply.started":"2023-12-11T00:08:44.731739Z","shell.execute_reply":"2023-12-11T00:08:45.855271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the model\n\n#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n# fit model\nmodel.fit([X1train, X2train], ytrain, epochs=200,batch_size =512, verbose=1, validation_data=([X1dev, X2dev], ydev))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:11:43.509635Z","iopub.execute_input":"2023-12-11T00:11:43.511722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n \n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n    # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=0)\n        # convert probability to integer\n        yhat = np.argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text","metadata":{"id":"hLPll5Cy9NqD","execution":{"iopub.status.busy":"2023-12-11T00:03:43.740188Z","iopub.status.idle":"2023-12-11T00:03:43.740497Z","shell.execute_reply.started":"2023-12-11T00:03:43.740331Z","shell.execute_reply":"2023-12-11T00:03:43.740353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\n# evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n    actual, predicted = list(), list()\n    # step over the whole set\n    for key, desc_list in descriptions.items():\n        # generate description\n        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n        # store actual and predicted\n        references = [d.split() for d in desc_list]\n        actual.append(references)\n        predicted.append(yhat.split())\n    # calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))","metadata":{"id":"FJxjSWfkAtBl","execution":{"iopub.status.busy":"2023-12-11T00:03:43.741557Z","iopub.status.idle":"2023-12-11T00:03:43.741897Z","shell.execute_reply.started":"2023-12-11T00:03:43.741692Z","shell.execute_reply":"2023-12-11T00:03:43.741716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing dataset","metadata":{}},{"cell_type":"code","source":"# load test set\nfilename = '../input/d/sayanf/flickr8k/Flickr8k_text/Flickr_8k.testImages.txt'\ntest = load_set(filename)\nprint('Dataset: %d' % len(test))\n# descriptions\ntest_descriptions = load_clean_descriptions('./descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\n# photo features\ntest_features = load_photo_features('./features.pkl', test)\nprint('Photos: test=%d' % len(test_features))","metadata":{"id":"Z6UMzhbiAs3V","execution":{"iopub.status.busy":"2023-12-11T00:03:43.743653Z","iopub.status.idle":"2023-12-11T00:03:43.744154Z","shell.execute_reply.started":"2023-12-11T00:03:43.743921Z","shell.execute_reply":"2023-12-11T00:03:43.743945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\n# save the tokenizer\ndump(tokenizer, open('tokenizer.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T00:03:43.745118Z","iopub.status.idle":"2023-12-11T00:03:43.745539Z","shell.execute_reply.started":"2023-12-11T00:03:43.745298Z","shell.execute_reply":"2023-12-11T00:03:43.745320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.models import load_model\n\n# # load the model\n# filename = './model-ep020-loss2.890-val_loss4.751.h5'\n# model = load_model(filename)\n# evaluate model\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"id":"IPAd9dL2Asua","execution":{"iopub.status.busy":"2023-12-11T00:03:43.746869Z","iopub.status.idle":"2023-12-11T00:03:43.747155Z","shell.execute_reply.started":"2023-12-11T00:03:43.747003Z","shell.execute_reply":"2023-12-11T00:03:43.747018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load and prepare the photograph\nphoto = extract_features('../input/example-img')\n# generate description\nmax_length = 17\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(description)","metadata":{"id":"kpkeHIkLCpYP","execution":{"iopub.status.busy":"2023-12-11T00:03:43.748501Z","iopub.status.idle":"2023-12-11T00:03:43.748826Z","shell.execute_reply.started":"2023-12-11T00:03:43.748637Z","shell.execute_reply":"2023-12-11T00:03:43.748661Z"},"trusted":true},"execution_count":null,"outputs":[]}]}